/Users/daniel/Programming/PythonProjects/RubikSolver/.venv/bin/python /Users/daniel/Programming/PythonProjects/RubikSolver/.venv/src/RubikMainAlgo.py
===================== Setting Up Data =====================
Train States: (616841, 54)
Train Moves: (616841, 2)
Test States: (10000, 54)
Test Moves: (10000, 2)
===================== Creating Datasets =====================
Success!
===================== Testing Datasets =====================
Train row index 1: (tensor([ 0.0000, -1.0000, -0.3333, -0.3333, -1.0000, -0.3333, -1.0000,  0.3333,
         0.6667,  0.3333, -1.0000, -1.0000,  0.6667, -0.6667,  0.0000,  0.0000,
        -0.6667, -0.3333, -1.0000,  0.6667, -0.6667, -0.6667, -0.3333,  0.3333,
         0.3333,  0.3333, -0.6667, -0.6667, -0.3333, -0.3333,  0.0000,  0.0000,
         0.6667,  0.6667,  0.3333,  0.0000, -1.0000, -1.0000,  0.6667, -0.6667,
         0.3333, -1.0000,  0.3333,  0.0000,  0.3333,  0.0000, -0.3333, -0.3333,
        -0.6667,  0.6667,  0.0000,  0.6667,  0.6667, -0.6667]), tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000, -1.0312], dtype=torch.float64))
Test row index 1: (tensor([-0.3333, -1.0000, -0.3333, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
        -1.0000,  0.3333,  0.6667,  0.6667, -0.6667, -0.6667,  0.3333, -1.0000,
        -0.6667,  0.6667, -0.6667, -0.3333, -0.6667, -0.3333, -0.3333, -0.3333,
        -0.3333, -0.3333, -0.3333,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.3333, -0.6667, -1.0000,  0.3333,
         0.3333,  0.3333,  0.3333,  0.3333,  0.3333, -0.6667, -0.6667, -0.6667,
         0.6667,  0.6667,  0.6667,  0.6667,  0.6667,  0.6667]), tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,
        0.0000, 0.1875], dtype=torch.float64))
Praise God!
===================== Creating NN Object =====================
Using cpu device
RubikNN(
  (layers): Sequential(
    (0): Linear(in_features=54, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=128, bias=True)
    (3): ReLU()
    (4): Linear(in_features=128, out_features=20, bias=True)
  )
)
===================== Training =====================
[------------- EPOCH 1 -------------]
loss: 0.072796  [   64/616841]
loss: 0.047021  [64064/616841]
loss: 0.047775  [128064/616841]
loss: 0.049290  [192064/616841]
loss: 0.051145  [256064/616841]
loss: 0.047993  [320064/616841]
loss: 0.045452  [384064/616841]
loss: 0.042816  [448064/616841]
loss: 0.046427  [512064/616841]
loss: 0.046580  [576064/616841]
Test Error:
 Accuracy: 21.5%, Avg loss: 0.043271

[------------- EPOCH 2 -------------]
loss: 0.041708  [   64/616841]
loss: 0.041492  [64064/616841]
loss: 0.043070  [128064/616841]
loss: 0.042582  [192064/616841]
loss: 0.042961  [256064/616841]
loss: 0.039872  [320064/616841]
loss: 0.040079  [384064/616841]
loss: 0.037613  [448064/616841]
loss: 0.038393  [512064/616841]
loss: 0.040430  [576064/616841]
Test Error:
 Accuracy: 23.3%, Avg loss: 0.041142

[------------- EPOCH 3 -------------]
loss: 0.042529  [   64/616841]
loss: 0.040775  [64064/616841]
loss: 0.040463  [128064/616841]
loss: 0.041025  [192064/616841]
loss: 0.040069  [256064/616841]
loss: 0.040081  [320064/616841]
loss: 0.037439  [384064/616841]
loss: 0.040020  [448064/616841]
loss: 0.041288  [512064/616841]
loss: 0.040112  [576064/616841]
Test Error:
 Accuracy: 25.1%, Avg loss: 0.039812

[------------- EPOCH 4 -------------]
loss: 0.037340  [   64/616841]
loss: 0.042120  [64064/616841]
loss: 0.039573  [128064/616841]
loss: 0.039991  [192064/616841]
loss: 0.038971  [256064/616841]
loss: 0.041107  [320064/616841]
loss: 0.038832  [384064/616841]
loss: 0.037297  [448064/616841]
loss: 0.039870  [512064/616841]
loss: 0.038583  [576064/616841]
Test Error:
 Accuracy: 29.2%, Avg loss: 0.038846

[------------- EPOCH 5 -------------]
loss: 0.038927  [   64/616841]
loss: 0.038333  [64064/616841]
loss: 0.037057  [128064/616841]
loss: 0.042725  [192064/616841]
loss: 0.039176  [256064/616841]
loss: 0.037988  [320064/616841]
loss: 0.041219  [384064/616841]
loss: 0.035193  [448064/616841]
loss: 0.038199  [512064/616841]
loss: 0.035476  [576064/616841]
Test Error:
 Accuracy: 29.3%, Avg loss: 0.038074

[------------- EPOCH 6 -------------]
loss: 0.038323  [   64/616841]
loss: 0.036410  [64064/616841]
loss: 0.035803  [128064/616841]
loss: 0.039257  [192064/616841]
loss: 0.034931  [256064/616841]
loss: 0.042057  [320064/616841]
loss: 0.038782  [384064/616841]
loss: 0.037840  [448064/616841]
loss: 0.039942  [512064/616841]
loss: 0.038850  [576064/616841]
Test Error:
 Accuracy: 31.1%, Avg loss: 0.037537

[------------- EPOCH 7 -------------]
loss: 0.037288  [   64/616841]
loss: 0.041626  [64064/616841]
loss: 0.037160  [128064/616841]
loss: 0.037113  [192064/616841]
loss: 0.035568  [256064/616841]
loss: 0.038422  [320064/616841]
loss: 0.035319  [384064/616841]
loss: 0.037349  [448064/616841]
loss: 0.035295  [512064/616841]
loss: 0.036655  [576064/616841]
Test Error:
 Accuracy: 32.0%, Avg loss: 0.036967

[------------- EPOCH 8 -------------]
loss: 0.036041  [   64/616841]
loss: 0.034725  [64064/616841]
loss: 0.035463  [128064/616841]
loss: 0.035170  [192064/616841]
loss: 0.037168  [256064/616841]
loss: 0.037858  [320064/616841]
loss: 0.035406  [384064/616841]
loss: 0.034954  [448064/616841]
loss: 0.034775  [512064/616841]
loss: 0.037930  [576064/616841]
Test Error:
 Accuracy: 32.0%, Avg loss: 0.036562

[------------- EPOCH 9 -------------]
loss: 0.033786  [   64/616841]
loss: 0.037093  [64064/616841]
loss: 0.037913  [128064/616841]
loss: 0.040806  [192064/616841]
loss: 0.036611  [256064/616841]
loss: 0.035339  [320064/616841]
loss: 0.035899  [384064/616841]
loss: 0.037789  [448064/616841]
loss: 0.034454  [512064/616841]
loss: 0.033665  [576064/616841]
Test Error:
 Accuracy: 31.0%, Avg loss: 0.036138

[------------- EPOCH 10 -------------]
loss: 0.037573  [   64/616841]
loss: 0.030521  [64064/616841]
loss: 0.039458  [128064/616841]
loss: 0.037160  [192064/616841]
loss: 0.036351  [256064/616841]
loss: 0.034954  [320064/616841]
loss: 0.035967  [384064/616841]
loss: 0.036292  [448064/616841]
loss: 0.033508  [512064/616841]
loss: 0.033577  [576064/616841]
Test Error:
 Accuracy: 33.3%, Avg loss: 0.035844

[------------- EPOCH 11 -------------]
loss: 0.034213  [   64/616841]
loss: 0.034721  [64064/616841]
loss: 0.037657  [128064/616841]
loss: 0.036284  [192064/616841]
loss: 0.034518  [256064/616841]
loss: 0.035477  [320064/616841]
loss: 0.034484  [384064/616841]
loss: 0.039615  [448064/616841]
loss: 0.031298  [512064/616841]
loss: 0.038346  [576064/616841]
Test Error:
 Accuracy: 33.9%, Avg loss: 0.035529

[------------- EPOCH 12 -------------]
loss: 0.034281  [   64/616841]
loss: 0.036983  [64064/616841]
loss: 0.034597  [128064/616841]
loss: 0.032570  [192064/616841]
loss: 0.035496  [256064/616841]
loss: 0.036257  [320064/616841]
loss: 0.037917  [384064/616841]
loss: 0.034540  [448064/616841]
loss: 0.034798  [512064/616841]
loss: 0.035253  [576064/616841]
Test Error:
 Accuracy: 33.1%, Avg loss: 0.035312

[------------- EPOCH 13 -------------]
loss: 0.035848  [   64/616841]
loss: 0.032643  [64064/616841]
loss: 0.039091  [128064/616841]
loss: 0.034180  [192064/616841]
loss: 0.036453  [256064/616841]
loss: 0.033448  [320064/616841]
loss: 0.038464  [384064/616841]
loss: 0.035428  [448064/616841]
loss: 0.038736  [512064/616841]
loss: 0.036043  [576064/616841]
Test Error:
 Accuracy: 33.2%, Avg loss: 0.035114

[------------- EPOCH 14 -------------]
loss: 0.035484  [   64/616841]
loss: 0.032006  [64064/616841]
loss: 0.035830  [128064/616841]
loss: 0.034785  [192064/616841]
loss: 0.035305  [256064/616841]
loss: 0.038679  [320064/616841]
loss: 0.038560  [384064/616841]
loss: 0.033918  [448064/616841]
loss: 0.032411  [512064/616841]
loss: 0.030888  [576064/616841]
Test Error:
 Accuracy: 33.6%, Avg loss: 0.034832

[------------- EPOCH 15 -------------]
loss: 0.035858  [   64/616841]
loss: 0.042050  [64064/616841]
loss: 0.037939  [128064/616841]
loss: 0.038162  [192064/616841]
loss: 0.037652  [256064/616841]
loss: 0.039922  [320064/616841]
loss: 0.033799  [384064/616841]
loss: 0.031219  [448064/616841]
loss: 0.036058  [512064/616841]
loss: 0.034430  [576064/616841]
Test Error:
 Accuracy: 35.7%, Avg loss: 0.034685

[------------- EPOCH 16 -------------]
loss: 0.031839  [   64/616841]
loss: 0.033395  [64064/616841]
loss: 0.031838  [128064/616841]
loss: 0.037635  [192064/616841]
loss: 0.033865  [256064/616841]
loss: 0.041148  [320064/616841]
loss: 0.033682  [384064/616841]
loss: 0.034214  [448064/616841]
loss: 0.032625  [512064/616841]
loss: 0.032027  [576064/616841]
Test Error:
 Accuracy: 35.0%, Avg loss: 0.034571

[------------- EPOCH 17 -------------]
loss: 0.030742  [   64/616841]
loss: 0.030647  [64064/616841]
loss: 0.034811  [128064/616841]
loss: 0.033457  [192064/616841]
loss: 0.037828  [256064/616841]
loss: 0.034948  [320064/616841]
loss: 0.034000  [384064/616841]
loss: 0.034851  [448064/616841]
loss: 0.032828  [512064/616841]
loss: 0.033603  [576064/616841]
Test Error:
 Accuracy: 34.6%, Avg loss: 0.034436

[------------- EPOCH 18 -------------]
loss: 0.032324  [   64/616841]
loss: 0.036700  [64064/616841]
loss: 0.035078  [128064/616841]
loss: 0.037222  [192064/616841]
loss: 0.031930  [256064/616841]
loss: 0.035873  [320064/616841]
loss: 0.032822  [384064/616841]
loss: 0.032271  [448064/616841]
loss: 0.031339  [512064/616841]
loss: 0.036924  [576064/616841]
Test Error:
 Accuracy: 35.2%, Avg loss: 0.034215

[------------- EPOCH 19 -------------]
loss: 0.030337  [   64/616841]
loss: 0.039108  [64064/616841]
loss: 0.030892  [128064/616841]
loss: 0.033701  [192064/616841]
loss: 0.030820  [256064/616841]
loss: 0.036017  [320064/616841]
loss: 0.032437  [384064/616841]
loss: 0.038869  [448064/616841]
loss: 0.037131  [512064/616841]
loss: 0.036380  [576064/616841]
Test Error:
 Accuracy: 35.0%, Avg loss: 0.034063

[------------- EPOCH 20 -------------]
loss: 0.039278  [   64/616841]
loss: 0.039105  [64064/616841]
loss: 0.034465  [128064/616841]
loss: 0.035772  [192064/616841]
loss: 0.032344  [256064/616841]
loss: 0.033304  [320064/616841]
loss: 0.035377  [384064/616841]
loss: 0.036465  [448064/616841]
loss: 0.030272  [512064/616841]
loss: 0.040885  [576064/616841]
Test Error:
 Accuracy: 38.3%, Avg loss: 0.033849

[------------- EPOCH 21 -------------]
loss: 0.033837  [   64/616841]
loss: 0.035855  [64064/616841]
loss: 0.034660  [128064/616841]
loss: 0.033384  [192064/616841]
loss: 0.032173  [256064/616841]
loss: 0.036833  [320064/616841]
loss: 0.036902  [384064/616841]
loss: 0.031806  [448064/616841]
loss: 0.032873  [512064/616841]
loss: 0.032339  [576064/616841]
Test Error:
 Accuracy: 38.4%, Avg loss: 0.033729

[------------- EPOCH 22 -------------]
loss: 0.031931  [   64/616841]
loss: 0.035356  [64064/616841]
loss: 0.036417  [128064/616841]
loss: 0.034730  [192064/616841]
loss: 0.032345  [256064/616841]
loss: 0.038753  [320064/616841]
loss: 0.030065  [384064/616841]
loss: 0.032804  [448064/616841]
loss: 0.032059  [512064/616841]
loss: 0.037196  [576064/616841]
Test Error:
 Accuracy: 38.3%, Avg loss: 0.033695

[------------- EPOCH 23 -------------]
loss: 0.034779  [   64/616841]
loss: 0.030748  [64064/616841]
loss: 0.030382  [128064/616841]
loss: 0.036663  [192064/616841]
loss: 0.032743  [256064/616841]
loss: 0.034488  [320064/616841]
loss: 0.036799  [384064/616841]
loss: 0.031776  [448064/616841]
loss: 0.030515  [512064/616841]
loss: 0.030830  [576064/616841]
Test Error:
 Accuracy: 37.4%, Avg loss: 0.033434

[------------- EPOCH 24 -------------]
loss: 0.035180  [   64/616841]
loss: 0.033928  [64064/616841]
loss: 0.033323  [128064/616841]
loss: 0.033068  [192064/616841]
loss: 0.032967  [256064/616841]
loss: 0.034368  [320064/616841]
loss: 0.031484  [384064/616841]
loss: 0.030938  [448064/616841]
loss: 0.035742  [512064/616841]
loss: 0.031991  [576064/616841]
Test Error:
 Accuracy: 39.5%, Avg loss: 0.033372

[------------- EPOCH 25 -------------]
loss: 0.036101  [   64/616841]
loss: 0.034252  [64064/616841]
loss: 0.030578  [128064/616841]
loss: 0.032211  [192064/616841]
loss: 0.028774  [256064/616841]
loss: 0.034724  [320064/616841]
loss: 0.034556  [384064/616841]
loss: 0.036793  [448064/616841]
loss: 0.030271  [512064/616841]
loss: 0.029759  [576064/616841]
Test Error:
 Accuracy: 39.5%, Avg loss: 0.033166

[------------- EPOCH 26 -------------]
loss: 0.030599  [   64/616841]
loss: 0.033179  [64064/616841]
loss: 0.036037  [128064/616841]
loss: 0.034515  [192064/616841]
loss: 0.031645  [256064/616841]
loss: 0.034503  [320064/616841]
loss: 0.032411  [384064/616841]
loss: 0.034682  [448064/616841]
loss: 0.034843  [512064/616841]
loss: 0.037399  [576064/616841]
Test Error:
 Accuracy: 40.8%, Avg loss: 0.033108

[------------- EPOCH 27 -------------]
loss: 0.035485  [   64/616841]
loss: 0.030723  [64064/616841]
loss: 0.036083  [128064/616841]
loss: 0.033358  [192064/616841]
loss: 0.030844  [256064/616841]
loss: 0.033143  [320064/616841]
loss: 0.031154  [384064/616841]
loss: 0.030032  [448064/616841]
loss: 0.036970  [512064/616841]
loss: 0.037441  [576064/616841]
Test Error:
 Accuracy: 40.8%, Avg loss: 0.032997

[------------- EPOCH 28 -------------]
loss: 0.032744  [   64/616841]
loss: 0.034283  [64064/616841]
loss: 0.034766  [128064/616841]
loss: 0.032658  [192064/616841]
loss: 0.034517  [256064/616841]
loss: 0.036345  [320064/616841]
loss: 0.033898  [384064/616841]
loss: 0.028748  [448064/616841]
loss: 0.031155  [512064/616841]
loss: 0.029432  [576064/616841]
Test Error:
 Accuracy: 40.8%, Avg loss: 0.032801

[------------- EPOCH 29 -------------]
loss: 0.036364  [   64/616841]
loss: 0.030332  [64064/616841]
loss: 0.036299  [128064/616841]
loss: 0.029133  [192064/616841]
loss: 0.034073  [256064/616841]
loss: 0.030407  [320064/616841]
loss: 0.031400  [384064/616841]
loss: 0.030070  [448064/616841]
loss: 0.029175  [512064/616841]
loss: 0.032352  [576064/616841]
Test Error:
 Accuracy: 38.5%, Avg loss: 0.032759

[------------- EPOCH 30 -------------]
loss: 0.032061  [   64/616841]
loss: 0.032387  [64064/616841]
loss: 0.033863  [128064/616841]
loss: 0.033868  [192064/616841]
loss: 0.030293  [256064/616841]
loss: 0.037774  [320064/616841]
loss: 0.030969  [384064/616841]
loss: 0.032554  [448064/616841]
loss: 0.033437  [512064/616841]
loss: 0.032953  [576064/616841]
Test Error:
 Accuracy: 41.2%, Avg loss: 0.032635

[------------- EPOCH 31 -------------]
loss: 0.036026  [   64/616841]
loss: 0.028021  [64064/616841]
loss: 0.033595  [128064/616841]
loss: 0.034576  [192064/616841]
loss: 0.026944  [256064/616841]
loss: 0.029261  [320064/616841]
loss: 0.032974  [384064/616841]
loss: 0.025838  [448064/616841]
loss: 0.031972  [512064/616841]
loss: 0.033968  [576064/616841]
Test Error:
 Accuracy: 38.8%, Avg loss: 0.032554

[------------- EPOCH 32 -------------]
loss: 0.029670  [   64/616841]
loss: 0.034008  [64064/616841]
loss: 0.029759  [128064/616841]
loss: 0.036440  [192064/616841]
loss: 0.028600  [256064/616841]
loss: 0.031561  [320064/616841]
loss: 0.033144  [384064/616841]
loss: 0.030238  [448064/616841]
loss: 0.034823  [512064/616841]
loss: 0.034421  [576064/616841]
Test Error:
 Accuracy: 41.8%, Avg loss: 0.032452

[------------- EPOCH 33 -------------]
loss: 0.033092  [   64/616841]
loss: 0.031409  [64064/616841]
loss: 0.034687  [128064/616841]
loss: 0.033983  [192064/616841]
loss: 0.030782  [256064/616841]
loss: 0.031064  [320064/616841]
loss: 0.033064  [384064/616841]
loss: 0.032582  [448064/616841]
loss: 0.029515  [512064/616841]
loss: 0.028465  [576064/616841]
Test Error:
 Accuracy: 40.8%, Avg loss: 0.032366

[------------- EPOCH 34 -------------]
loss: 0.029605  [   64/616841]
loss: 0.031352  [64064/616841]
loss: 0.033253  [128064/616841]
loss: 0.036145  [192064/616841]
loss: 0.034106  [256064/616841]
loss: 0.029548  [320064/616841]
loss: 0.035553  [384064/616841]
loss: 0.031806  [448064/616841]
loss: 0.033499  [512064/616841]
loss: 0.032645  [576064/616841]
Test Error:
 Accuracy: 41.2%, Avg loss: 0.032253

[------------- EPOCH 35 -------------]
loss: 0.033256  [   64/616841]
loss: 0.033856  [64064/616841]
loss: 0.029720  [128064/616841]
loss: 0.034817  [192064/616841]
loss: 0.033284  [256064/616841]
loss: 0.030665  [320064/616841]
loss: 0.031764  [384064/616841]
loss: 0.033358  [448064/616841]
loss: 0.035059  [512064/616841]
loss: 0.026458  [576064/616841]
Test Error:
 Accuracy: 42.5%, Avg loss: 0.032250

[------------- EPOCH 36 -------------]
loss: 0.035003  [   64/616841]
loss: 0.030505  [64064/616841]
loss: 0.032874  [128064/616841]
loss: 0.034801  [192064/616841]
loss: 0.031110  [256064/616841]
loss: 0.034348  [320064/616841]
loss: 0.032207  [384064/616841]
loss: 0.032383  [448064/616841]
loss: 0.029924  [512064/616841]
loss: 0.031452  [576064/616841]
Test Error:
 Accuracy: 39.6%, Avg loss: 0.032237

[------------- EPOCH 37 -------------]
loss: 0.032857  [   64/616841]
loss: 0.031628  [64064/616841]
loss: 0.033671  [128064/616841]
loss: 0.033427  [192064/616841]
loss: 0.033277  [256064/616841]
loss: 0.032824  [320064/616841]
loss: 0.032429  [384064/616841]
loss: 0.033665  [448064/616841]
loss: 0.032995  [512064/616841]
loss: 0.037388  [576064/616841]
Test Error:
 Accuracy: 40.6%, Avg loss: 0.031996

[------------- EPOCH 38 -------------]
loss: 0.029210  [   64/616841]
loss: 0.032882  [64064/616841]
loss: 0.031380  [128064/616841]
loss: 0.031768  [192064/616841]
loss: 0.032191  [256064/616841]
loss: 0.033126  [320064/616841]
loss: 0.029349  [384064/616841]
loss: 0.031713  [448064/616841]
loss: 0.030514  [512064/616841]
loss: 0.035867  [576064/616841]
Test Error:
 Accuracy: 44.4%, Avg loss: 0.032015

[------------- EPOCH 39 -------------]
loss: 0.031948  [   64/616841]
loss: 0.031615  [64064/616841]
loss: 0.029322  [128064/616841]
loss: 0.032642  [192064/616841]
loss: 0.033027  [256064/616841]
loss: 0.032470  [320064/616841]
loss: 0.029828  [384064/616841]
loss: 0.030887  [448064/616841]
loss: 0.030930  [512064/616841]
loss: 0.029216  [576064/616841]
Test Error:
 Accuracy: 42.7%, Avg loss: 0.031836

[------------- EPOCH 40 -------------]
loss: 0.031627  [   64/616841]
loss: 0.028592  [64064/616841]
loss: 0.029488  [128064/616841]
loss: 0.029610  [192064/616841]
loss: 0.031834  [256064/616841]
loss: 0.033360  [320064/616841]
loss: 0.035252  [384064/616841]
loss: 0.035167  [448064/616841]
loss: 0.028962  [512064/616841]
loss: 0.028570  [576064/616841]
Test Error:
 Accuracy: 41.4%, Avg loss: 0.031703

[------------- EPOCH 41 -------------]
loss: 0.037402  [   64/616841]
loss: 0.028167  [64064/616841]
loss: 0.029631  [128064/616841]
loss: 0.033438  [192064/616841]
loss: 0.030200  [256064/616841]
loss: 0.032172  [320064/616841]
loss: 0.033817  [384064/616841]
loss: 0.031047  [448064/616841]
loss: 0.032183  [512064/616841]
loss: 0.036763  [576064/616841]
Test Error:
 Accuracy: 43.4%, Avg loss: 0.031690

[------------- EPOCH 42 -------------]
loss: 0.031901  [   64/616841]
loss: 0.031722  [64064/616841]
loss: 0.031686  [128064/616841]
loss: 0.032234  [192064/616841]
loss: 0.034402  [256064/616841]
loss: 0.033369  [320064/616841]
loss: 0.031658  [384064/616841]
loss: 0.029005  [448064/616841]
loss: 0.031108  [512064/616841]
loss: 0.030223  [576064/616841]
Test Error:
 Accuracy: 42.8%, Avg loss: 0.031580

[------------- EPOCH 43 -------------]
loss: 0.036517  [   64/616841]
loss: 0.031501  [64064/616841]
loss: 0.033445  [128064/616841]
loss: 0.029514  [192064/616841]
loss: 0.034036  [256064/616841]
loss: 0.031418  [320064/616841]
loss: 0.028434  [384064/616841]
loss: 0.030530  [448064/616841]
loss: 0.030908  [512064/616841]
loss: 0.033163  [576064/616841]
Test Error:
 Accuracy: 44.6%, Avg loss: 0.031553

[------------- EPOCH 44 -------------]
loss: 0.030893  [   64/616841]
loss: 0.030958  [64064/616841]
loss: 0.032437  [128064/616841]
loss: 0.032688  [192064/616841]
loss: 0.031227  [256064/616841]
loss: 0.031006  [320064/616841]
loss: 0.031537  [384064/616841]
loss: 0.031821  [448064/616841]
loss: 0.032266  [512064/616841]
loss: 0.031363  [576064/616841]
Test Error:
 Accuracy: 41.2%, Avg loss: 0.031510

[------------- EPOCH 45 -------------]
loss: 0.032424  [   64/616841]
loss: 0.031245  [64064/616841]
loss: 0.031969  [128064/616841]
loss: 0.032719  [192064/616841]
loss: 0.031850  [256064/616841]
loss: 0.032022  [320064/616841]
loss: 0.032164  [384064/616841]
loss: 0.028044  [448064/616841]
loss: 0.028817  [512064/616841]
loss: 0.036813  [576064/616841]
Test Error:
 Accuracy: 42.5%, Avg loss: 0.031385

[------------- EPOCH 46 -------------]
loss: 0.030531  [   64/616841]
loss: 0.033445  [64064/616841]
loss: 0.033435  [128064/616841]
loss: 0.034088  [192064/616841]
loss: 0.032350  [256064/616841]
loss: 0.033691  [320064/616841]
loss: 0.028695  [384064/616841]
loss: 0.031458  [448064/616841]
loss: 0.030431  [512064/616841]
loss: 0.034583  [576064/616841]
Test Error:
 Accuracy: 41.7%, Avg loss: 0.031373

[------------- EPOCH 47 -------------]
loss: 0.029187  [   64/616841]
loss: 0.031671  [64064/616841]
loss: 0.033258  [128064/616841]
loss: 0.028527  [192064/616841]
loss: 0.030725  [256064/616841]
loss: 0.031442  [320064/616841]
loss: 0.034793  [384064/616841]
loss: 0.031652  [448064/616841]
loss: 0.030319  [512064/616841]
loss: 0.033407  [576064/616841]
Test Error:
 Accuracy: 44.6%, Avg loss: 0.031356

[------------- EPOCH 48 -------------]
loss: 0.032139  [   64/616841]
loss: 0.030754  [64064/616841]
loss: 0.030966  [128064/616841]
loss: 0.031254  [192064/616841]
loss: 0.033652  [256064/616841]
loss: 0.035385  [320064/616841]
loss: 0.030631  [384064/616841]
loss: 0.031200  [448064/616841]
loss: 0.031231  [512064/616841]
loss: 0.032855  [576064/616841]
Test Error:
 Accuracy: 42.8%, Avg loss: 0.031273

[------------- EPOCH 49 -------------]
loss: 0.032811  [   64/616841]
loss: 0.030790  [64064/616841]
loss: 0.031502  [128064/616841]
loss: 0.032849  [192064/616841]
loss: 0.031237  [256064/616841]
loss: 0.029047  [320064/616841]
loss: 0.027063  [384064/616841]
loss: 0.029849  [448064/616841]
loss: 0.035361  [512064/616841]
loss: 0.036379  [576064/616841]
Test Error:
 Accuracy: 43.1%, Avg loss: 0.031240

[------------- EPOCH 50 -------------]
loss: 0.032333  [   64/616841]
loss: 0.031514  [64064/616841]
loss: 0.029891  [128064/616841]
loss: 0.027689  [192064/616841]
loss: 0.031694  [256064/616841]
loss: 0.029330  [320064/616841]
loss: 0.028986  [384064/616841]
loss: 0.030178  [448064/616841]
loss: 0.030958  [512064/616841]
loss: 0.034430  [576064/616841]
Test Error:
 Accuracy: 45.0%, Avg loss: 0.031180

Done :)

Process finished with exit code 0
